{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "import pytorch_lightning as pl\n",
    "from torchvision.ops import sigmoid_focal_loss, batched_nms\n",
    "\n",
    "from modules.utils import convert_to_xywh, convert_to_xyxy, generate_subset, calc_iou\n",
    "from modules.datasets import CocoDetection\n",
    "import modules.transforms as T\n",
    "from modules.models import RetinaNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def post_process(\n",
    "    preds_class: torch.Tensor,\n",
    "    preds_box: torch.Tensor,\n",
    "    anchors: torch.Tensor,\n",
    "    targets: list[dict],\n",
    "    conf_threshold: float = 0.05,\n",
    "    nms_threshold: float = 0.5,\n",
    "):\n",
    "    batch_size = preds_class.shape[0]\n",
    "\n",
    "    anchors_xywh = convert_to_xywh(anchors)\n",
    "\n",
    "    preds_box[:, :, :2] = anchors_xywh[:, :2] + preds_box[:, :, :2] * anchors_xywh[:, 2:]\n",
    "    preds_box[:, :, 2:] = preds_box[:, :, 2:].exp() * anchors_xywh[:, 2:]\n",
    "\n",
    "    preds_box = convert_to_xyxy(preds_box)\n",
    "\n",
    "    preds_class = preds_class.sigmoid()\n",
    "\n",
    "    scores = []\n",
    "    labels = []\n",
    "    boxes = []\n",
    "    for img_preds_class, img_preds_box, img_targets in zip(preds_class, preds_box, targets):\n",
    "        # Clamp bounding box into image size.\n",
    "        img_preds_box[:, ::2] = img_preds_box[:, ::2].clamp(min=0, max=img_targets[\"img_size\"][0]) # x\n",
    "        img_preds_box[:, 1::2] = img_preds_box[:, 1::2].clamp(min=0, max=img_targets[\"img_size\"][1]) # y\n",
    "\n",
    "        # Rescale bounding box to fit with original image.\n",
    "        img_preds_box *= img_targets[\"orig_img_size\"][0] / img_targets[\"img_size\"][0]\n",
    "\n",
    "        img_preds_score, img_preds_label = img_preds_class.max(dim=1)\n",
    "\n",
    "        keep = img_preds_score > conf_threshold\n",
    "        img_preds_score = img_preds_score[keep]\n",
    "        img_preds_label = img_preds_label[keep]\n",
    "        img_preds_box = img_preds_box[keep]\n",
    "\n",
    "        # Apply NMS per class\n",
    "        keep_indices = batched_nms(img_preds_box, img_preds_score, img_preds_label, nms_threshold)\n",
    "        scores.append(img_preds_score[keep_indices])\n",
    "        labels.append(img_preds_label[keep_indices])\n",
    "        boxes.append(img_preds_box[keep_indices])\n",
    "\n",
    "    return scores, labels, boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(\n",
    "    preds_class: torch.Tensor,\n",
    "    preds_box: torch.Tensor,\n",
    "    anchors: torch.Tensor,\n",
    "    targets: list[dict],\n",
    "    iou_lower_threshold: float = 0.4,\n",
    "    iou_upper_threshold: float = 0.5,\n",
    "):\n",
    "    \"\"\"Compute Focal Loss.\n",
    "    Args:\n",
    "        preds_class (Tensor[N, num_anchors, num_classes]): Classes.\n",
    "        preds_box (Tensor[N, num_anchors, 4]): Bounding boxes.\n",
    "            Coordinate should be (x_diff, y_diff, w_diff, h_diff).\n",
    "        anchors (Tensor[num_anchors, 4]): Coordinate should be (xmin, ymin, xmax, ymax).\n",
    "        targets: Labels.\n",
    "    \"\"\"\n",
    "    anchors_xywh = convert_to_xywh(anchors)\n",
    "\n",
    "    # Calculate target function per image\n",
    "    loss_class = preds_class.new_tensor(0)\n",
    "    loss_box = preds_box.new_tensor(0)\n",
    "    for img_preds_class, img_preds_box, img_targets in zip(\n",
    "        preds_class, preds_box, targets\n",
    "    ):\n",
    "        # If no ground truth for this image.\n",
    "        if img_targets[\"classes\"].shape[0] == 0:\n",
    "            # Create target class as background.\n",
    "            targets_class = torch.zeros_like(img_preds_class)\n",
    "            loss_class += sigmoid_focal_loss(\n",
    "                img_preds_class, targets_class, reduction=\"sum\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        # Get a bounding box which has max IoU.\n",
    "        ious = calc_iou(anchors, img_targets[\"boxes\"])[0]\n",
    "        ious_max, ious_argmax = ious.max(dim=1)\n",
    "\n",
    "        # Init class label as -1.\n",
    "        # Set label of anchor box as -1 if iou_lower_threshold <= IoU <= iou_upper_threshold\n",
    "        # in order not to calculate loss.\n",
    "        targets_class = torch.full_like(img_preds_class, -1)\n",
    "\n",
    "        targets_class[ious_max < iou_lower_threshold] = 0\n",
    "\n",
    "        # If IoU > iou_upper_threshold, set as classification/regression target.\n",
    "        positive_masks = ious_max > iou_upper_threshold\n",
    "        num_positive_anchors = positive_masks.sum()\n",
    "\n",
    "        targets_class[positive_masks] = 0\n",
    "        assigned_classes = img_targets[\"classes\"][ious_argmax]\n",
    "        targets_class[positive_masks, assigned_classes[positive_masks]] = 1\n",
    "\n",
    "        loss_class += (\n",
    "            (targets_class != -1) * sigmoid_focal_loss(img_preds_class, targets_class)\n",
    "        ).sum() / num_positive_anchors.clamp(min=1)\n",
    "\n",
    "        # If no positive anchors, skip calculation of loss_box\n",
    "        if num_positive_anchors == 0:\n",
    "            continue\n",
    "\n",
    "        # Get ground truth per anchor\n",
    "        assgined_boxes = img_targets[\"boxes\"][ious_argmax]\n",
    "        assgined_boxes_xywh = convert_to_xywh(assgined_boxes)\n",
    "\n",
    "        targets_box = torch.zeros_like(img_preds_box)\n",
    "        targets_box[:, :2] = assgined_boxes_xywh[:, :2] - anchors_xywh[:, :2] / anchors_xywh[:, 2:]\n",
    "        targets_box[:, 2:] = (assgined_boxes_xywh[:, 2:] / assgined_boxes_xywh[:, 2:]).log()\n",
    "\n",
    "        loss_box += F.smooth_l1_loss(img_preds_box[positive_masks], targets_box[positive_masks], beta=1/9)\n",
    "\n",
    "    batch_size = preds_class.shape[0]\n",
    "    loss_class /= batch_size\n",
    "    loss_box /= batch_size\n",
    "\n",
    "    return loss_class, loss_box\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    img_dir: str = \"./data/coco/val2014\"\n",
    "    annot_file: str = \"./data/coco/instances_val2014_small.json\"\n",
    "    save_file: str = \"./workdir/model/retinanet.pth\"\n",
    "\n",
    "    train_ratio: float = 0.8\n",
    "    num_epochs: int = 50\n",
    "    lr_drop: int = 45\n",
    "    val_interval: int = 1\n",
    "    lr: float = 1e-5\n",
    "    clip: float = 0.1\n",
    "    moving_avg: int = 100\n",
    "    batch_size: int = 8\n",
    "    num_workers: int = 4\n",
    "    device: str = \"cuda\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    max_height = 0\n",
    "    max_width = 0\n",
    "    for img, _ in batch:\n",
    "        h, w = img.shape[1:]\n",
    "        max_height = max(max_height, h)\n",
    "        max_width = max(max_width, w)\n",
    "\n",
    "    height = (max_height + 31) // 32 * 32\n",
    "    width = (max_width + 31) // 32 * 32\n",
    "\n",
    "    imgs = batch[0][0].new_zeros((len(batch), 3, height, width))\n",
    "    targets = []\n",
    "    for i, (img, target) in enumerate(batch):\n",
    "        h, w = img.shape[1:]\n",
    "        imgs[i, :, :h, :w] = img\n",
    "        targets.append(target)\n",
    "\n",
    "    return imgs, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(\n",
    "    loader, model, loss_fn, conf_threshold=0.05, nms_threshold=0.5, device=\"cuda\"\n",
    "):\n",
    "    model.eval()\n",
    "\n",
    "    loss_class_list = []\n",
    "    loss_box_list = []\n",
    "    loss_list = []\n",
    "    preds = []\n",
    "    img_ids = []\n",
    "    for imgs, targets in tqdm(loader, desc=\"[Validation]\"):\n",
    "        imgs = imgs.to(device)\n",
    "        targets = [{k: v.to(device) for k, v in target.items()} for target in targets]\n",
    "\n",
    "        preds_class, preds_box, anchors = model(imgs)\n",
    "\n",
    "        loss_class, loss_box = loss_fn(preds_class, preds_box, anchors, targets)\n",
    "        loss = loss_class + loss_box\n",
    "\n",
    "        loss_class_list.append(loss_class)\n",
    "        loss_box_list.append(loss_box)\n",
    "        loss_list.append(loss)\n",
    "\n",
    "        scores, labels, boxes = post_process(\n",
    "            preds_class, preds_box, anchors, targets, conf_threshold, nms_threshold\n",
    "        )\n",
    "        for img_scores, img_labels, img_boxes, img_targets in zip(\n",
    "            scores, labels, boxes, targets\n",
    "        ):\n",
    "            img_ids.append(img_targets[\"image_id\"].item())\n",
    "\n",
    "            # To xywh\n",
    "            img_boxes[:, 2:] -= img_boxes[:, :2]\n",
    "\n",
    "            for score, label, box in zip(img_scores, img_labels, img_boxes):\n",
    "                preds.append(\n",
    "                    {\n",
    "                        \"image_id\": img_targets[\"image_id\"].item(),\n",
    "                        \"category_id\": loader.dataset.to_coco_label(label.item()),\n",
    "                        \"score\": score.item(),\n",
    "                        \"bbox\": box.to(\"cpu\").numpy().tolist(),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    loss_class = torch.stack(loss_class_list).mean().item()\n",
    "    loss_box = torch.stack(loss_box_list).mean().item()\n",
    "    loss = torch.stack(loss_list).mean().item()\n",
    "    print(\n",
    "        f\"Validation loss = {loss:.3f}, class loss = {loss_class:.3f}, box loss = {loss_box:.3f}\"\n",
    "    )\n",
    "\n",
    "    if len(preds) == 0:\n",
    "        print(\"Nothing detected, skip evaluation.\")\n",
    "        return\n",
    "\n",
    "    with open(\"tmp.json\", \"w\") as f:\n",
    "        json.dump(preds, f)\n",
    "\n",
    "    coco_results = loader.dataset.coco.loadRes(\"tmp.json\")\n",
    "\n",
    "    coco_eval = COCOeval(loader.dataset.coco, coco_results, \"bbox\")\n",
    "    coco_eval.params.imgIds = img_ids\n",
    "    coco_eval.evaluate()\n",
    "    coco_eval.accumulate()\n",
    "    coco_eval.summarize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(config: Config):\n",
    "    # Data augmentation\n",
    "    min_sizes = (480, 512, 544, 576, 608)\n",
    "    train_transform = T.Compose(\n",
    "        [\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.RandomSelect(\n",
    "                T.RandomResize(min_sizes, max_size=1024),\n",
    "                T.Compose(\n",
    "                    [\n",
    "                        T.RandomSizeCrop(scale=(0.8, 1.0), ratio=(0.75, 1.333)),\n",
    "                        T.RandomResize(min_sizes, max_size=1024),\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    test_transform = T.Compose(\n",
    "        [\n",
    "            T.RandomResize([min_sizes[-1]], max_size=1024),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    train_dataset = CocoDetection(\n",
    "        config.img_dir, config.annot_file, transform=train_transform\n",
    "    )\n",
    "    val_dataset = CocoDetection(\n",
    "        config.img_dir, config.annot_file, transform=test_transform\n",
    "    )\n",
    "\n",
    "    train_set, val_set = generate_subset(train_dataset, config.train_ratio)\n",
    "    print(f\"num of train samples\", len(train_set))\n",
    "    print(f\"num of val samples\", len(val_set))\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        num_workers=config.num_workers,\n",
    "        sampler=SubsetRandomSampler(train_set),\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        num_workers=config.num_workers,\n",
    "        sampler=val_set,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "\n",
    "    model = RetinaNet(len(train_dataset.classes))\n",
    "    torch.compile(model)\n",
    "    model.to(config.device)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config.lr)\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[config.lr_drop], gamma=0.1)\n",
    "\n",
    "    for epoch in range(config.num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        with tqdm(train_loader) as pbar:\n",
    "            pbar.set_description(f\"[Epoch {epoch + 1}]\")\n",
    "\n",
    "            loss_class_hist = deque()\n",
    "            loss_box_hist = deque()\n",
    "            loss_hist = deque()\n",
    "            for imgs, targets in pbar:\n",
    "                imgs = imgs.to(config.device)\n",
    "                targets = [{k: v.to(config.device)for k, v in target.items()} for target in targets]\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                preds_class, preds_box, anchors = model(imgs)\n",
    "                loss_class, loss_box = loss_fn(preds_class, preds_box, anchors, targets)\n",
    "                loss = loss_class + loss_box\n",
    "\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad.clip_grad_norm_(model.parameters(), config.clip)\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                loss_class_hist.append(loss_class.item())\n",
    "                loss_box_hist.append(loss_box.item())\n",
    "                loss_hist.append(loss.item())\n",
    "                if len(loss_hist) > config.moving_avg:\n",
    "                    loss_class_hist.popleft()\n",
    "                    loss_box_hist.popleft()\n",
    "                    loss_hist.popleft()\n",
    "                pbar.set_postfix({\n",
    "                    \"loss\": torch.Tensor(loss_hist).mean().item(),\n",
    "                    \"loss_class\": torch.Tensor(loss_class_hist).mean().item(),\n",
    "                    \"loss_box\": torch.Tensor(loss_box_hist).mean().item(),\n",
    "                })\n",
    "        scheduler.step()\n",
    "\n",
    "        torch.save(model.state_dict(), config.save_file)\n",
    "\n",
    "        if (epoch + 1) % config.val_interval == 0:\n",
    "            evaluate(val_loader, model, loss_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoDetDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self, root_dir, annot_file, batch_size=16, train_ratio: float = 0.8, num_workers: int = 4\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.root_dir = root_dir\n",
    "        self.annot_file = annot_file\n",
    "        self.batch_size = batch_size\n",
    "        self.train_ratio = train_ratio\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        min_sizes = (480, 512, 544, 576, 608)\n",
    "        self.train_transform = T.Compose(\n",
    "            [\n",
    "                T.RandomHorizontalFlip(),\n",
    "                T.RandomSelect(\n",
    "                    T.RandomResize(min_sizes, max_size=1024),\n",
    "                    T.Compose(\n",
    "                        [\n",
    "                            T.RandomSizeCrop(scale=(0.8, 1.0), ratio=(0.75, 1.333)),\n",
    "                            T.RandomResize(min_sizes, max_size=1024),\n",
    "                        ]\n",
    "                    ),\n",
    "                ),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "            ]\n",
    "        )\n",
    "        self.test_transform = T.Compose(\n",
    "            [\n",
    "                T.RandomResize([min_sizes[-1]], max_size=1024),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def setup(self, stage: str) -> None:\n",
    "        self.train_dataset = CocoDetection(self.root_dir, self.annot_file, self.train_transform)\n",
    "        self.train_set, self.val_set = generate_subset(self.train_dataset, self.train_ratio)\n",
    "        self.test_dataset = CocoDetection(self.root_dir, self.annot_file, self.test_transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            sampler=SubsetRandomSampler(self.train_set),\n",
    "            collate_fn=collate_fn,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            sampler=self.val_set,\n",
    "            collate_fn=collate_fn,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            sampler=self.val_set,\n",
    "            collate_fn=collate_fn,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetinaNetModule(pl.LightningModule):\n",
    "    def __init__(self, num_classes:int, learning_rate:float, lr_drop:float):\n",
    "        super().__init__()\n",
    "        self.model = RetinaNet(num_classes)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lr_drop = lr_drop\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[self.lr_drop], gamma=0.1)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        imgs, targets = batch\n",
    "        preds_class, preds_box, anchors = self.model(imgs)\n",
    "        loss_class, loss_box = loss_fn(preds_class, preds_box, anchors, targets)\n",
    "        loss = loss_class + loss_box\n",
    "        self.log(\"train_loss_class\", loss_class)\n",
    "        self.log(\"train_loss_box\", loss_box)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        imgs, targets = batch\n",
    "        preds_class, preds_box, anchors = self.model(imgs)\n",
    "        loss_class, loss_box = loss_fn(preds_class, preds_box, anchors, targets)\n",
    "        loss = loss_class + loss_box\n",
    "        self.log(\"val_loss_class\", loss_class)\n",
    "        self.log(\"val_loss_box\", loss_box)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.48s)                                    \n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.44s)\n",
      "creating index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type      | Params\n",
      "------------------------------------\n",
      "0 | model | RetinaNet | 19.8 M\n",
      "------------------------------------\n",
      "19.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "19.8 M    Total params\n",
      "79.168    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index created!\n",
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00<00:00,  1.37it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 8. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  10%|▉         | 96/1000 [00:56<08:48,  1.71it/s, v_num=4]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "config = Config()\n",
    "data_module = CocoDetDataModule(config.img_dir, config.annot_file, config.batch_size, config.train_ratio, config.num_workers)\n",
    "model_module = RetinaNetModule(num_classes=2, learning_rate=config.lr, lr_drop=config.lr_drop)\n",
    "trainer = pl.Trainer(accelerator=\"gpu\", max_epochs=10)\n",
    "trainer.fit(model_module, datamodule=data_module)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
