{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "from dataclasses import dataclass\n",
    "from collections import deque\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "import pytorch_lightning as pl\n",
    "from torchvision.ops import sigmoid_focal_loss, batched_nms\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from PIL import Image\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "from modules.utils import convert_to_xywh, convert_to_xyxy, generate_subset, calc_iou, collate_fn\n",
    "from modules.datasets import CocoDetection\n",
    "import modules.transforms as T\n",
    "from modules.models import RetinaNet\n",
    "from modules.model_orig import RetinaNet as RetinaNetOrig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from retinanet import post_process, get_loader, Config, loss_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.31s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=1.32s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "config = Config()\n",
    "train_loader, val_loader = get_loader(config.img_dir, config.annot_file, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=[], unexpected_keys=['fc.weight', 'fc.bias'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RetinaNet(2)\n",
    "model_orig = RetinaNetOrig(2)\n",
    "model_orig.backbone.load_state_dict(torch.hub.load_state_dict_from_url('https://download.pytorch.org/models/resnet18-5c106cde.pth'), strict=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, target = next(iter(train_loader))\n",
    "out = model(img)\n",
    "out_orig = model_orig(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 65295, 2]), torch.Size([1, 65295, 4]), torch.Size([65295, 4]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].shape, out[1].shape, out[2].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 65295, 2]), torch.Size([1, 65295, 4]), torch.Size([65295, 4]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_orig[0].shape, out_orig[1].shape, out_orig[2].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.1287, grad_fn=<DivBackward0>),\n",
       " tensor(0.1166, grad_fn=<DivBackward0>))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(out[0], out[1], out[2], target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.1287, grad_fn=<DivBackward0>),\n",
       " tensor(0.1166, grad_fn=<DivBackward0>))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(out_orig[0], out_orig[1], out_orig[2], target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.1287, grad_fn=<DivBackward0>),\n",
       " tensor(0.1166, grad_fn=<DivBackward0>))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(out[0], out[1], out[2], target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.1287, grad_fn=<DivBackward0>),\n",
       " tensor(0.1166, grad_fn=<DivBackward0>))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(out_orig[0], out_orig[1], out_orig[2], target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(preds_class: torch.Tensor, preds_box: torch.Tensor,\n",
    "              anchors: torch.Tensor, targets: dict,\n",
    "              iou_lower_threshold: float=0.4,\n",
    "              iou_upper_threshold: float=0.5):\n",
    "    anchors_xywh = convert_to_xywh(anchors)\n",
    "\n",
    "    # 画像毎に目的関数を計算\n",
    "    loss_class = preds_class.new_tensor(0)\n",
    "    loss_box = preds_class.new_tensor(0)\n",
    "    for img_preds_class, img_preds_box, img_targets in zip(\n",
    "            preds_class, preds_box, targets):\n",
    "        # 現在の画像に対する正解矩形がないとき\n",
    "        if img_targets['classes'].shape[0] == 0:\n",
    "            # 全ての物体クラスの確率が0となるように\n",
    "            # (背景として分類されるように)ラベルを作成\n",
    "            targets_class = torch.zeros_like(img_preds_class)\n",
    "            loss_class += sigmoid_focal_loss(\n",
    "                img_preds_class, targets_class, reduction='sum')\n",
    "\n",
    "            continue\n",
    "\n",
    "        # 各画素のアンカーボックスと正解矩形のIoUを計算し、\n",
    "        # 各アンカーボックスに対して最大のIoUを持つ正解矩形を抽出\n",
    "        ious = calc_iou(anchors, img_targets['boxes'])[0]\n",
    "        ious_max, ious_argmax = ious.max(dim=1)\n",
    "\n",
    "        # 分類のラベルを-1で初期化\n",
    "        # IoUが下の閾値と上の閾値の間にあるアンカーボックスは\n",
    "        # ラベルを-1として損失を計算しないようにする\n",
    "        targets_class = torch.full_like(img_preds_class, -1)\n",
    "\n",
    "        # アンカーボックスとマッチした正解矩形のIoUが下の閾値より\n",
    "        # 小さい場合、全ての物体クラスの確率が0となるようラベルを用意\n",
    "        targets_class[ious_max < iou_lower_threshold] = 0\n",
    "\n",
    "        # アンカーボックスとマッチした正解矩形のIoUが上の閾値より\n",
    "        # 大きい場合、陽性のアンカーボックスとして分類回帰の対象にする\n",
    "        positive_masks = ious_max > iou_upper_threshold\n",
    "        num_positive_anchors = positive_masks.sum()\n",
    "\n",
    "        # 陽性のアンカーボックスについて、マッチした正解矩形が示す\n",
    "        # 物体クラスの確率を1、それ以外を0として出力するように\n",
    "        # ラベルに値を代入\n",
    "        targets_class[positive_masks] = 0\n",
    "        assigned_classes = img_targets['classes'][ious_argmax]\n",
    "        targets_class[positive_masks,\n",
    "                      assigned_classes[positive_masks]] = 1\n",
    "\n",
    "        # IoUが下の閾値と上の閾値の間にあるアンカーボックスについては\n",
    "        # 分類の損失を計算しない\n",
    "        loss_class += ((targets_class != -1) * sigmoid_focal_loss(\n",
    "            img_preds_class, targets_class)).sum() / \\\n",
    "            num_positive_anchors.clamp(min=1)\n",
    "\n",
    "        # 陽性のアンカーボックスが一つも存在しないとき\n",
    "        # 矩形の誤差の学習はしない\n",
    "        if num_positive_anchors == 0:\n",
    "            continue\n",
    "\n",
    "        # 各アンカーボックスにマッチした正解矩形を抽出\n",
    "        assigned_boxes = img_targets['boxes'][ious_argmax]\n",
    "        assigned_boxes_xywh = convert_to_xywh(assigned_boxes)\n",
    "\n",
    "        # アンカーボックスとマッチした正解矩形との誤差を計算し、\n",
    "        # ラベルを作成\n",
    "        targets_box = torch.zeros_like(img_preds_box)\n",
    "        # 中心位置の誤差はアンカーボックスの大きさでスケール\n",
    "        targets_box[:, :2] = (\n",
    "            assigned_boxes_xywh[:, :2] - anchors_xywh[:, :2]) / \\\n",
    "            anchors_xywh[:, 2:]\n",
    "        # 大きさはアンカーボックスに対するスケールのlogを予測\n",
    "        targets_box[:, 2:] = (assigned_boxes_xywh[:, 2:] / \\\n",
    "                              anchors_xywh[:, 2:]).log()\n",
    "\n",
    "        # L1誤差とL2誤差を組み合わせたsmooth L1誤差を使用\n",
    "        loss_box += F.smooth_l1_loss(img_preds_box[positive_masks],\n",
    "                                     targets_box[positive_masks],\n",
    "                                     beta=1 / 9)\n",
    "\n",
    "    batch_size = preds_class.shape[0]\n",
    "    loss_class = loss_class / batch_size\n",
    "    loss_box = loss_box / batch_size\n",
    "\n",
    "    return loss_class, loss_box\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([147312, 4])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[2].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 147312, 4])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[1].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 147312, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "model =timm.create_model(\"resnet18\", features_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128, 256, 512]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.feature_info.channels()[-3:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
